# -*- coding: utf-8 -*-
"""Wed_Progress(03July) (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QGWcV_17W-ci9M5Uid1FeEVKGo7fPBqR
"""

import pandas as pd
import seaborn as sns
from matplotlib import pyplot as pyplot
import sklearn

Breast_dataframe = pd.read_csv('/content/sample_data/breast-cancer (1).csv')

pd.set_option('display.max_rows', None)  # Show all rows
pd.set_option('display.max_columns', None)  # Show all columns

# Assuming 'df' is your DataFrame
print(Breast_dataframe)

# prompt: display the whole dataset

print(Breast_dataframe)

Breast_dataframe.head()

Breast_dataframe.describe()

Breast_dataframe.columns

print(Breast_dataframe.isnull().sum())

non_numeric_cols = Breast_dataframe.select_dtypes(exclude=['number']).columns
print(non_numeric_cols)

#Converting the Diagnosis Data type, 1 for Malignant(Cancerous tumor) and 0 for Benign(Non-cancerous tumor).

Breast_dataframe['diagnosis'] = Breast_dataframe['diagnosis'].map({'M': 1, 'B': 0})

Breast_dataframe.info()

"""# **Analysis of relationships using graphs**

# **The relationships of the features**
"""

#Import libraries for plotting graphs
import matplotlib.pyplot as plot
import seaborn as sns



#1.HISTOGRAM
#Computing the graph
features = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean',
    'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',
    'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se',
    'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se',
    'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst',
    'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst']

features_dataframe=Breast_dataframe[features]

features_dataframe.hist(figsize=(15, 15))

#Displaying the Graph
print("HISTOGRAM OF ALL FEATURES")
plot.show()

print("")
print("")



#2.Histogram with the selected features

#Computing the graph
SelectedFeatures =['concave points_worst', 'texture_mean', 'perimeter_mean', 'area_mean',
    'smoothness_mean', 'concavity_mean','compactness_mean' ,
    'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',
    'radius_worst', 'perimeter_worst', 'area_worst', 'radius_mean']



Selected_Relevant_Features_df = Breast_dataframe[SelectedFeatures]

Selected_Relevant_Features_df.hist(figsize=(15, 15))

#Displaying the Graph
print("HISTOGRAM OF ALL RELEVANT FEATURES")
plot.show()
print("")
print("")


#3.Box Plots
#3.1.Box Plots of all features
features_dataframe.plot(kind='box', subplots=True, layout=(6, 5), figsize=(15, 12), sharex=False, sharey=False)
print("BOX PLOT OF all FEATURES")
plot.show()
print("")
print("")



#3.2.Box Plots of Relevant features
Selected_Relevant_Features_df.plot(kind='box', subplots=True, layout=(5, 5), figsize=(15, 10), sharex=False, sharey=False)
print("BOX PLOT OF ALL RELEVANT FEATURES")
plot.show()
print("")
print("")


#4.Pair Plots

#For all features
sns.pairplot(Breast_dataframe, hue='diagnosis')
print("PAIR PLOT for all features")
plot.suptitle("The Pair Plots for all features")
plot.show()


#Pair plot of the relevant features only
sns.pairplot(Selected_Relevant_Features_df, kind='reg')
print("PAIR PLOT for relevant features")
plot.suptitle("The Pair Plots for the selected relevant features")
plot.show()

"""# **Feature selection using correlation matrix**"""

# Computing the correlation matrix for checking the relevant features
Correlation_matrix = Breast_dataframe.corr()

# Plotting the graph
pyplot.figure(figsize=(15, 13))
sns.heatmap(Correlation_matrix, annot=True, cmap='coolwarm', fmt = '.2f')

#Add a title to the plot
pyplot.title('Correlation Matrix of features')

#Display the Plot
pyplot.show()

#Identify the highly correlated features to the Diagnosis feature

correlation_threshold= 0.5
high_correlation_features = Correlation_matrix['diagnosis'][abs(Correlation_matrix['diagnosis'] > correlation_threshold)]

#Remove the Diagnosis variable from the list of features
high_correlation_features = high_correlation_features.drop(['diagnosis'])

#Printing the features
print("The features that are highly correlated to the diagnosis variable ")
print(high_correlation_features)

"""# **FEATURE SELECTION PROCESS, using Ftest and mutual infomation through SelectKBest library**"""

#Import the libraries
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif

#Separate the features variable and target variable
features = Breast_dataframe.drop(['id','diagnosis'], axis=1)
target = Breast_dataframe['diagnosis']

# Apply SelectKBest with ANOVA F-test method.
select_k_best_anova = SelectKBest(score_func=f_classif, k=10)
selected_features_anova = select_k_best_anova.fit_transform(features, target)
selected_features_anova_df = pd.DataFrame(selected_features_anova, columns=features.columns[select_k_best_anova.get_support()])

#Displaying the features using Anova F-test with SelectKBest library
print("Selected features using ANOVA F-test:\n", selected_features_anova_df.head())


# # Apply SelectKBest with Mutual Information
select_k_best_mi = SelectKBest(score_func=mutual_info_classif, k=10)
selected_features_mi = select_k_best_mi.fit_transform(features, target)
selected_features_mi_df = pd.DataFrame(selected_features_mi, columns=features.columns[select_k_best_mi.get_support()])

#Displaying the features using Mutual Information with SelectKBest library
print("Selected features using Mutual Information:\n", selected_features_mi_df.head())

"""Combined Relevant Selected Features :
radius_mean,
texture_mean,
perimeter_mean,
area_mean,
smoothness_mean,
compactness_mean,
concavity_mean,
concave points_mean,
symmetry_mean,
fractal_dimension_mean,
radius_worst,
perimeter_worst,
area_worst,
concave points_worst

# ** Data spliting And Feature Scalling**
"""

#Importing library for  spliting the dataset and Standardizing the features
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


#List of selected relevant features ( 14 seleceted relevant features)
#New dataset, Diagnosis is excluded from this list because it is the target variable
selected_Relevant_Features =['concave points_worst', 'texture_mean', 'perimeter_mean', 'area_mean',
    'smoothness_mean', 'concavity_mean','compactness_mean' ,
    'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',
    'radius_worst', 'perimeter_worst', 'area_worst', 'radius_mean']

#Displaying the selected features
print("Selected features",selected_Relevant_Features )
print("   ")
print("   ")
print("   ")
print("   ")

#Separating the selected relevant features from the target feature(Diagnosis)
new_features =Breast_dataframe[selected_Relevant_Features]        #The list of new features in the dataset
target_variable = Breast_dataframe['diagnosis']                    #The target variable in the dataset


#Spliting the dataset into 80% training set and 20% testing set.
features_train, features_test, target_train, target_test = train_test_split (new_features, target_variable, test_size = 0.2, random_state=42)

#Displaying each feature variable and the data
print("Features training set \n",features_train)
print("   ")
print("   ")
print("   ")
print("   ")

print("Features testing set \n",features_test)
print("   ")
print("   ")
print("   ")
print("   ")

print("Target training set \n",target_train)
print("   ")
print("   ")
print("   ")
print("   ")

print("Target testing set \n",target_test)
print("   ")
print("   ")
print("   ")
print("   ")


#Feature scaling process

#Creating a Standard Scaler Object.
scalerObject= StandardScaler()

#Fitting and Transforming the features data
features_Scaled_training_set = scalerObject.fit_transform(features_train)  #Uses combined method fit_transform, calculates the statistics(mean, standard deviatiion) to scale the training set
features_Scaled_testing_set = scalerObject.transform(features_test)        #Uses only transform method to scale data, the data will be scaled using the previous statistics calculated


#Displaying the new Scaled training and testing data for the features
print("New Scaled Training Set \n",features_Scaled_training_set)
print("   ")
print("   ")
print("   ")
print("   ")
print("New Scaled Testing Set \n",features_Scaled_testing_set)
print("   ")
print("   ")
print("   ")
print("   ")

"""# **MODEL TRAINING**

### ***1.Support Vector Machine Model.***
"""

#Support Vector Machine(SVM) model training
print("Support Vector Machine(SVM) model training and evaluation")
print("   ")
print("   ")

#Importing libraries
from sklearn.svm import SVC


#Create the SVM classifier object
SVM_Model_Classifier = SVC(kernel = 'linear' , C=1.0 , random_state = 42 , probability=True)

#Training the model
SVM_Model_Classifier.fit(features_Scaled_training_set, target_train)

#Detect and predict the outcome of a patient cancer status using the trained model
PredictedValues = SVM_Model_Classifier.predict(features_Scaled_testing_set)

#Displaying the Predicted Values
print("Predicted Values using the trained SVM model :",PredictedValues)
print("   ")
print("   ")




#Support Vector Machine(SVM) model evaluation

#Importing libraries
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report,confusion_matrix
from matplotlib import pyplot as plot
import seaborn as sns


#Initiliaze the predictor variable


#Calculating the Evaluation Metrics percentage
accuracyScore = accuracy_score (target_test, PredictedValues)
precisionScore = precision_score (target_test, PredictedValues)
recallScore = recall_score (target_test, PredictedValues)
f1Score = f1_score (target_test, PredictedValues)


#Printing the Value of the Evaluation Metrics
print("Accuracy Score percentange = ", accuracyScore)
print("Precision Score percentange = ", precisionScore)
print("Recall Score percentange = ", recallScore)
print("The F1 Score percentange = ", f1Score)
print("The overall Evaluation Report :")
print("   ")
print("   ")
print(classification_report(target_test, PredictedValues))
print("   ")
print("   ")

print("The Confusion matrix for the SVM model ")

#Computing the matrix
confusionMatrix=confusion_matrix(target_test, PredictedValues)

#plotting the matrix
sns.heatmap(confusionMatrix, annot=True, cmap='coolwarm', fmt = '.2f', xticklabels=['Benign' , 'Malignant'] , yticklabels=['Benign' , 'Malignant'])

#Add title and lables to the plot
plot.title('Confusion Matrix', fontsize=30)
plot.xlabel('Predicted Values',fontsize=15)
plot.gca().xaxis.set_label_position('top')
plot.ylabel('Actual Values' , fontsize=15)

#Setting the position of the xtick labels on the top
plot.gca().xaxis.tick_top()

#Display the Plot
plot.show()

"""## **`*2.RANDOM FOREST MODEL TRAINING*`**"""

#Random Forest(RF) model training
print("Random Forest(RF) model training and evaluation")
print("   ")
print("   ")

#Importing libraries
from sklearn.ensemble import RandomForestClassifier

#Create the RF classifier object
RF_Model_Classifier = RandomForestClassifier(n_estimators = 250, random_state = 42, probability = True)

#Training the model
RF_Model_Classifier.fit(features_Scaled_training_set, target_train)

#Detect and predict the outcome of a patient cancer status using the trained model
PredictedValues = RF_Model_Classifier.predict(features_Scaled_testing_set)

#Displaying the Predicted Values
print("Predicted Values using the trained RF model :",PredictedValues)
print("   ")
print("   ")




#Random Forest(RF) model evaluation


#Calculating the Evaluation Metrics percentage
accuracyScore = accuracy_score (target_test, PredictedValues)
precisionScore = precision_score (target_test, PredictedValues)
recallScore = recall_score (target_test, PredictedValues)
f1Score = f1_score (target_test, PredictedValues)


#Printing the Value of the Evaluation Metrics
print("Accuracy Score percentange = ", accuracyScore)
print("Precision Score percentange = ", precisionScore)
print("Recall Score percentange = ", recallScore)
print("The F1 Score percentange = ", f1Score)
print("   ")
print("   ")
print("The overall Evaluation Report :")
print("   ")

print(classification_report(target_test, PredictedValues))
print("   ")
print("   ")


print("The Confusion matrix for the RF model ")

#Computing the matrix
confusionMatrix=confusion_matrix(target_test, PredictedValues)

#plotting the matrix
sns.heatmap(confusionMatrix, annot=True, cmap='coolwarm', fmt = '.2f', xticklabels=['Benign' , 'Malignant'] , yticklabels=['Benign' , 'Malignant'])

#Add title and lables to the plot
plot.title('Confusion Matrix', fontsize=30)
plot.xlabel('Predicted Values',fontsize=15)
plot.gca().xaxis.set_label_position('top')
plot.ylabel('Actual Values' , fontsize=15)

#Setting the position of the xtick labels on the top
plot.gca().xaxis.tick_top()

#Display the Plot
plot.show()

"""## ***`3.K-NEARREST NEIGHBOR MODEL TRAINING`***"""

#K-Nearest Neigbhor(KNN) model training
print("K-Nearest Neigbhor(KNN) model training and evaluation")
print("   ")
print("   ")

#Importing libraries
from sklearn.neighbors import KNeighborsClassifier

#Create the KNN classifier object
KNN_Model_Classifier = KNeighborsClassifier(n_neighbors = 10, weights= 'distance')

#Training the model
KNN_Model_Classifier.fit(features_Scaled_training_set, target_train)

#Detect and predict the outcome of a patient cancer status using the trained model
PredictedValues = KNN_Model_Classifier.predict(features_Scaled_testing_set)

#Displaying the Predicted Values
print("Predicted Values using the trained KNN model :",PredictedValues)
print("   ")
print("   ")




#K-Nearest Neighbor(KNN) model evaluation


#Calculating the Evaluation Metrics percentage
accuracyScore = accuracy_score (target_test, PredictedValues)
precisionScore = precision_score (target_test, PredictedValues)
recallScore = recall_score (target_test, PredictedValues)
f1Score = f1_score (target_test, PredictedValues)


#Printing the Value of the Evaluation Metrics
print("Accuracy Score percentange = ", accuracyScore)
print("Precision Score percentange = ", precisionScore)
print("Recall Score percentange = ", recallScore)
print("The F1 Score percentange = ", f1Score)
print("   ")
print("   ")
print("The overall Evaluation Report :")
print("   ")

print(classification_report(target_test, PredictedValues))
print("   ")
print("   ")

print("The Confusion matrix for the KNN model ")

#Computing the matrix
confusionMatrix=confusion_matrix(target_test, PredictedValues)


#plotting the matrix
sns.heatmap(confusionMatrix, annot=True, cmap='coolwarm', fmt = '.2f', xticklabels=['Benign' , 'Malignant'] , yticklabels=['Benign' , 'Malignant'])

#Add title and lables to the plot
plot.title('Confusion Matrix', fontsize=30)
plot.xlabel('Predicted Values',fontsize=15)
plot.gca().xaxis.set_label_position('top')
plot.ylabel('Actual Values' , fontsize=15)

#Setting the position of the xtick labels on the top
plot.gca().xaxis.tick_top()

#Display the Plot
plot.show()

"""# **MODEL COMPARISON**

Three(3) models were trained:
### 1. Recall(True positive rate)
**WHY RECALL? Important for correctly diagnosing a patient, since this is the main aim of this research**

1.SVM = 0.9534883720930233
2.RF = 0.9302325581395349
3.KNN = 0.9534883720930233

2.Precision

2.1.SVM= 1.0
2.2.KNN= 1.0

Model selection:

SVM will be chosen because of the ability to work effectively even in a dataset with more features.

## **ROC CURVE**
"""

#For SVM

from sklearn.metrics import roc_curve, auc

# Get predicted probabilities for the positive class
y_pred_proba = SVM_Model_Classifier.predict_proba(features_Scaled_testing_set)[:, 1]

# Calculate the ROC curve
fpr, tpr, thresholds = roc_curve(target_test, y_pred_proba)

# Calculate the AUC
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plot.figure()
plot.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plot.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plot.xlim([0.0, 1.0])
plot.ylim([0.0, 1.05])
plot.text(0.5, 0.3, 'AUC = %0.2f' % roc_auc, fontsize=12, color='black')
plot.xlabel('False Positive Rate')
plot.ylabel('True Positive Rate')
plot.title('Receiver Operating Characteristic (ROC) Curve')
plot.legend(loc="lower right")
plot.show()

#For RF
# Get predicted probabilities for the positive class
y_pred_proba = RF_Model_Classifier.predict_proba(features_Scaled_testing_set)[:, 1]

# Calculate the ROC curve
fpr, tpr, thresholds = roc_curve(target_test, y_pred_proba)

# Calculate the AUC
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plot.figure()
plot.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plot.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plot.xlim([0.0, 1.0])
plot.ylim([0.0, 1.05])
plot.text(0.5, 0.3, 'AUC = %0.2f' % roc_auc, fontsize=12, color='black')
plot.xlabel('False Positive Rate')
plot.ylabel('True Positive Rate')
plot.title('Receiver Operating Characteristic (ROC) Curve')
plot.legend(loc="lower right")
plot.show()

# For KNN Model

# Get predicted probabilities for the positive class
y_pred_proba = KNN_Model_Classifier.predict_proba(features_Scaled_testing_set)[:, 1]

# Calculate the ROC curve
fpr, tpr, thresholds = roc_curve(target_test, y_pred_proba)

# Calculate the AUC
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plot.figure()
plot.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plot.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plot.xlim([0.0, 1.0])
plot.ylim([0.0, 1.05])
plot.text(0.5, 0.3, 'AUC = %0.2f' % roc_auc, fontsize=12, color='black')
plot.xlabel('False Positive Rate')
plot.ylabel('True Positive Rate')
plot.title('Receiver Operating Characteristic (ROC) Curve')
plot.legend(loc="lower right")
plot.show()

#Combined Classfication report

print("SVM Classification Report:")
print(classification_report(target_test, SVM_PredictedValues))

print("\nRandom Forest Classification Report:")
print(classification_report(target_test, RF_PredictedValues))

print("\nK-Nearest Neighbors Classification Report:")
print(classification_report(target_test, KNN_PredictedValues))